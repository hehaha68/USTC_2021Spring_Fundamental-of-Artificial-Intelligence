{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W1 = np.ones([4, 5])\n",
    "        self.W2 = np.ones([4, 4])\n",
    "        self.W3 = np.ones([3, 4])\n",
    "        self.b1 = np.ones([4,1])\n",
    "        self.b2 = np.ones([4, 1])\n",
    "        self.b3 = np.ones([3, 1])\n",
    "        self.alpha = 0.001  # 学习率\n",
    "        self.epsilon = 10e-6  # 阈值\n",
    "        self.epoch = 1000  # 最大迭代次数\n",
    "        self.h1 = np.zeros([4, 100])\n",
    "        self.h2 = np.zeros([4, 100])\n",
    "\n",
    "    def Sigmoid(self, x):\n",
    "        return 1 / (np.exp(-x) + 1)\n",
    "\n",
    "    def Diff_sigmoid(self, x):\n",
    "        y = self.Sigmoid(x)\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def Softmax(self, input):\n",
    "        exp = np.exp(input)\n",
    "        return exp / np.sum(exp, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def CrossEntropy(self, out, label):  # label属于one-hot向量(nx3)，out是输出神经元的向量(nx3)，返回交叉熵\n",
    "        log = np.log(out)\n",
    "        return np.trace(-log @ label.T) / len(label)\n",
    "\n",
    "    def FP(self, w1, w2, w3, b1, b2, b3,input):  # W1:(4x5) W2:(4x4) W3:(3x4)\n",
    "        self.h1 = f.Sigmoid(w1 @ input.T + b1)  # h1每列代表一行x的隐层\n",
    "        self.h2 = f.Sigmoid(w2 @ self.h1 + b2)  # h2每列代表h1的隐层\n",
    "        return f.Softmax((w3 @ self.h2 + b3).T)  # 输出nx3\n",
    "\n",
    "    def One_hot(self,label): #label:(nx1),返回nx3的one_hot矩阵\n",
    "        n = len(label)\n",
    "        t = np.zeros((n,3))\n",
    "        for i in range(n):\n",
    "            t[i,label[i]] = 1\n",
    "        return t\n",
    "\n",
    "    def GD(self,dw1,dw2,dw3,db1,db2,db3):\n",
    "        self.W1 = self.W1 - self.alpha * dw1\n",
    "        self.W2 = self.W2 - self.alpha * dw2\n",
    "        self.W3 = self.W3 - self.alpha * dw3\n",
    "        self.b1 = self.b1 - self.alpha * db1\n",
    "        self.b2 = self.b2 - self.alpha * db2\n",
    "        self.b3 = self.b3 - self.alpha * db3\n",
    "\n",
    "    def BP(self,train,w1,w2,w3,b1,b2,b3,label):  #label:(3xn)\n",
    "        yloss = (self.FP(w1,w2,w3,b1,b2,b3,train).T - label)/len(train)\n",
    "        dw3 = yloss @ self.h2.T    #d3:(3x4)\n",
    "        db3 = yloss @ np.ones([100,1])\n",
    "        t = (w3.T @ yloss) * self.Diff_sigmoid(w2 @ self.h1)   #t:(4xn)\n",
    "        dw2 = t @ self.h1.T  #d2:(4x4)\n",
    "        db2 = t @ np.ones([100,1])\n",
    "        tt = self.Diff_sigmoid(w1 @ train.T)\n",
    "        dw1 = (t * tt) @ train    #d1:(4x5)\n",
    "        db1 = (t * tt) @ np.ones([100,1])\n",
    "        return dw1,dw2,dw3,db1,db2,db3\n",
    "\n",
    "    def fit(self,train,label):  #train:(nx5),label:(nx1)\n",
    "        Loss = []\n",
    "        for i in range(self.epoch):\n",
    "            loss = self.CrossEntropy(self.FP(self.W1,self.W2,self.W3,self.b1,self.b2,self.b3,train),self.One_hot(label))\n",
    "            Loss.append(loss)\n",
    "            if len(Loss) < 2:\n",
    "                pass\n",
    "            elif abs(Loss[i]- Loss[i-1]) < self.epsilon:\n",
    "                break\n",
    "            dw1,dw2,dw3,db1,db2,db3 = self.BP(train,self.W1,self.W2,self.W3,self.b1,self.b2,self.b3,self.One_hot(label).T)\n",
    "            self.GD(dw1,dw2,dw3,db1,db2,db3)\n",
    "        print('mydi:',dw1,dw2,dw3)\n",
    "        print('mybi:',db1,db2,db3)\n",
    "        #self.plot(Loss)\n",
    "        print('myloss:',Loss[len(Loss)-1])\n",
    "        return Loss\n",
    "\n",
    "    def plot(self,L):\n",
    "        x = range(1, len(L)+1)\n",
    "        plt.title('损失函数随训练次数的变化')\n",
    "        plt.xlabel('训练次数')\n",
    "        plt.ylabel('损失函数')\n",
    "        plt.plot(x,L)\n",
    "        plt.show()\n",
    "    \n",
    "    def compare(self,x,y):\n",
    "        train = torch.from_numpy(x)\n",
    "        y = y.flatten()\n",
    "        label = torch.from_numpy(y)\n",
    "        w1 = torch.from_numpy(np.ones([4, 5]))\n",
    "        w2 = torch.from_numpy(np.ones([4, 4]))\n",
    "        w3 = torch.from_numpy(np.ones([3, 4]))\n",
    "        b1 = torch.from_numpy(np.ones([4,1]))\n",
    "        b2 = torch.from_numpy(np.ones([4, 1]))\n",
    "        b3 = torch.from_numpy(np.ones([3, 1]))\n",
    "        w1 = torch.autograd.Variable(w1,requires_grad=True)\n",
    "        w2 = torch.autograd.Variable(w2,requires_grad=True)\n",
    "        w3 = torch.autograd.Variable(w3,requires_grad=True)\n",
    "        b3 = torch.autograd.Variable(b3,requires_grad=True)\n",
    "        b2 = torch.autograd.Variable(b2,requires_grad=True)\n",
    "        b1 = torch.autograd.Variable(b1,requires_grad=True)\n",
    "        sig = nn.Sigmoid()\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "        Loss = []\n",
    "        for i in range(self.epoch):\n",
    "            h1 = sig(torch.mm(w1,train.T)+b1)\n",
    "            h2 = sig(torch.mm(w2,h1)+b2)\n",
    "            y_hat = torch.mm(w3,h2).T +b3.T\n",
    "            loss = ce(y_hat,label.long())\n",
    "            Loss.append(float(loss))\n",
    "            if len(Loss)< 2:\n",
    "                pass\n",
    "            elif abs(Loss[i]-Loss[i-1])< self.epsilon:\n",
    "                break\n",
    "            loss.backward()\n",
    "            d1 = w1.grad\n",
    "            d2 = w2.grad\n",
    "            d3 = w3.grad\n",
    "            db1 = b1.grad\n",
    "            db2 = b2.grad\n",
    "            db3 = b3.grad\n",
    "            w1 = torch.autograd.Variable(w1 - 1*self.alpha * d1,requires_grad=True)\n",
    "            w2 = torch.autograd.Variable(w2 - 1*self.alpha * d2,requires_grad=True)\n",
    "            w3 = torch.autograd.Variable(w3 - 1*self.alpha * d3,requires_grad=True)\n",
    "            b1 = torch.autograd.Variable(b1 - 1*self.alpha * db1,requires_grad=True)\n",
    "            b2 = torch.autograd.Variable(b2 - 1*self.alpha * db2,requires_grad=True)\n",
    "            b3 = torch.autograd.Variable(b3 - 1*self.alpha * db3,requires_grad=True)\n",
    "        print('torch di:',d1,d2,d3)\n",
    "        print('torch bi:',b1,b2,b3)\n",
    "        #self.plot(Loss)\n",
    "        print('torch_loss:',Loss[len(Loss)-1])\n",
    "        return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f=MLP()\n",
    "train_data = np.random.random((100,5)) * 100\n",
    "label = np.random.randint(3, size=(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydi: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[-2.24076587e-06 -2.24076587e-06 -2.24076587e-06 -2.24076587e-06]\n",
      " [-2.24076587e-06 -2.24076587e-06 -2.24076587e-06 -2.24076587e-06]\n",
      " [-2.24076587e-06 -2.24076587e-06 -2.24076587e-06 -2.24076587e-06]\n",
      " [-2.24076587e-06 -2.24076587e-06 -2.24076587e-06 -2.24076587e-06]] [[-0.03298118 -0.03298118 -0.03298118 -0.03298118]\n",
      " [ 0.02999268  0.02999268  0.02999268  0.02999268]\n",
      " [ 0.0029885   0.0029885   0.0029885   0.0029885 ]]\n",
      "mybi: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] [[-2.24076587e-06]\n",
      " [-2.24076587e-06]\n",
      " [-2.24076587e-06]\n",
      " [-2.24076587e-06]] [[-0.03320341]\n",
      " [ 0.03019477]\n",
      " [ 0.00300864]]\n",
      "myloss: 1.0979375989067373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0986122886681096,\n",
       " 1.0986000970247116,\n",
       " 1.098587945553988,\n",
       " 1.0985758341245444,\n",
       " 1.0985637626054086,\n",
       " 1.0985517308660322,\n",
       " 1.0985397387762872,\n",
       " 1.0985277862064646,\n",
       " 1.0985158730272748,\n",
       " 1.0985039991098455,\n",
       " 1.0984921643257188,\n",
       " 1.098480368546853,\n",
       " 1.0984686116456173,\n",
       " 1.0984568934947954,\n",
       " 1.0984452139675798,\n",
       " 1.0984335729375732,\n",
       " 1.0984219702787847,\n",
       " 1.0984104058656319,\n",
       " 1.098398879572937,\n",
       " 1.0983873912759266,\n",
       " 1.0983759408502296,\n",
       " 1.0983645281718777,\n",
       " 1.0983531531173012,\n",
       " 1.0983418155633307,\n",
       " 1.0983305153871945,\n",
       " 1.0983192524665173,\n",
       " 1.0983080266793182,\n",
       " 1.0982968379040117,\n",
       " 1.0982856860194046,\n",
       " 1.0982745709046948,\n",
       " 1.0982634924394719,\n",
       " 1.0982524505037132,\n",
       " 1.0982414449777835,\n",
       " 1.0982304757424366,\n",
       " 1.0982195426788093,\n",
       " 1.0982086456684232,\n",
       " 1.098197784593185,\n",
       " 1.0981869593353786,\n",
       " 1.098176169777674,\n",
       " 1.0981654158031164,\n",
       " 1.0981546972951304,\n",
       " 1.0981440141375187,\n",
       " 1.0981333662144592,\n",
       " 1.098122753410503,\n",
       " 1.098112175610577,\n",
       " 1.0981016326999775,\n",
       " 1.098091124564375,\n",
       " 1.0980806510898085,\n",
       " 1.0980702121626842,\n",
       " 1.0980598076697783,\n",
       " 1.0980494374982317,\n",
       " 1.0980391015355515,\n",
       " 1.098028799669609,\n",
       " 1.098018531788637,\n",
       " 1.0980082977812324,\n",
       " 1.09799809753635,\n",
       " 1.097987930943307,\n",
       " 1.097977797891775,\n",
       " 1.0979676982717883,\n",
       " 1.0979576319737325,\n",
       " 1.0979475988883498,\n",
       " 1.0979375989067373]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.fit(train_data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch di: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], dtype=torch.float64) tensor([[-8.4340e-07, -8.4340e-07, -8.4340e-07, -8.4340e-07],\n",
      "        [-8.4340e-07, -8.4340e-07, -8.4340e-07, -8.4340e-07],\n",
      "        [-8.4340e-07, -8.4340e-07, -8.4340e-07, -8.4340e-07],\n",
      "        [-8.4340e-07, -8.4340e-07, -8.4340e-07, -8.4340e-07]],\n",
      "       dtype=torch.float64) tensor([[-0.0330, -0.0330, -0.0330, -0.0330],\n",
      "        [ 0.0300,  0.0300,  0.0300,  0.0300],\n",
      "        [ 0.0030,  0.0030,  0.0030,  0.0030]], dtype=torch.float64)\n",
      "torch bi: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], dtype=torch.float64, requires_grad=True) tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], dtype=torch.float64, requires_grad=True) tensor([[1.0021],\n",
      "        [0.9981],\n",
      "        [0.9998]], dtype=torch.float64, requires_grad=True)\n",
      "torch_loss: 1.0979375989077267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0986122886681082,\n",
       " 1.098600097024711,\n",
       " 1.0985879455539875,\n",
       " 1.0985758341245455,\n",
       " 1.09856376260541,\n",
       " 1.0985517308660322,\n",
       " 1.098539738776286,\n",
       " 1.098527786206465,\n",
       " 1.0985158730272782,\n",
       " 1.098503999109848,\n",
       " 1.0984921643257237,\n",
       " 1.0984803685468585,\n",
       " 1.0984686116456261,\n",
       " 1.0984568934948065,\n",
       " 1.098445213967594,\n",
       " 1.0984335729375903,\n",
       " 1.0984219702788038,\n",
       " 1.0984104058656563,\n",
       " 1.0983988795729647,\n",
       " 1.0983873912759605,\n",
       " 1.0983759408502687,\n",
       " 1.098364528171925,\n",
       " 1.098353153117355,\n",
       " 1.0983418155633915,\n",
       " 1.0983305153872625,\n",
       " 1.098319252466595,\n",
       " 1.0983080266794054,\n",
       " 1.0982968379041083,\n",
       " 1.0982856860195132,\n",
       " 1.0982745709048143,\n",
       " 1.0982634924396049,\n",
       " 1.0982524505038587,\n",
       " 1.0982414449779427,\n",
       " 1.0982304757426105,\n",
       " 1.0982195426789985,\n",
       " 1.098208645668632,\n",
       " 1.0981977845934074,\n",
       " 1.0981869593356208,\n",
       " 1.0981761697779349,\n",
       " 1.098165415803396,\n",
       " 1.098154697295432,\n",
       " 1.0981440141378442,\n",
       " 1.0981333662148072,\n",
       " 1.098122753410875,\n",
       " 1.098112175610972,\n",
       " 1.0981016327004005,\n",
       " 1.0980911245648246,\n",
       " 1.0980806510902843,\n",
       " 1.098070212163191,\n",
       " 1.0980598076703159,\n",
       " 1.098049437498801,\n",
       " 1.0980391015361515,\n",
       " 1.0980287996702431,\n",
       " 1.098018531789306,\n",
       " 1.0980082977819376,\n",
       " 1.0979980975370929,\n",
       " 1.097987930944087,\n",
       " 1.0979777978925953,\n",
       " 1.0979676982726483,\n",
       " 1.0979576319746338,\n",
       " 1.097947598889293,\n",
       " 1.0979375989077267]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.compare(train_data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
